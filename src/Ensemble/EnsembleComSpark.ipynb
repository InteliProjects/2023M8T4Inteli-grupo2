{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recursos Necessários\n",
        "\n",
        "O programa abaixo foi executado no Colab, com os arquivos presentes na pasta \"pof_ipm_nm\" deste repositório."
      ],
      "metadata": {
        "id": "0nQqPbaB65Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGuOd9vy6o6W",
        "outputId": "7657b1ce-5ca3-4e23-d7fd-6133d5d5d04b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNWVXCjf6dHO",
        "outputId": "a5926cde-f0e7-4670-891c-8f7662bf75a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=5915da3afc81cc390e8b91e1ffa24b852491b3fa357540f90aed2a42e67c4164\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MNx5tbF-6dHS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8174ur0z6dHS"
      },
      "outputs": [],
      "source": [
        "pof6_2008_2009 = pd.read_excel('/content/drive/MyDrive/T4M8G2/ESSEMBLE/POF_IPM_NM/2008_2009/Tabela 6a.xlsx')\n",
        "pof6_2017_2018 = pd.read_excel('/content/drive/MyDrive/T4M8G2/ESSEMBLE/POF_IPM_NM/2017_2018/Tabela 6b.xlsx')\n",
        "pof6 = pd.concat([pof6_2008_2009, pof6_2017_2018], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pof6.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbtXuBHE6oE1",
        "outputId": "988da3c4-cee5-44fa-c296-c1a2904d0c5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 56 entries, 0 to 55\n",
            "Data columns (total 10 columns):\n",
            " #   Column                                              Non-Null Count  Dtype  \n",
            "---  ------                                              --------------  -----  \n",
            " 0   Unidades da Federação                               56 non-null     object \n",
            " 1   Proporção de pessoas das famílias residentes (%)    56 non-null     float64\n",
            " 2   Proporção de pessoas com algum grau de pobreza (%)  56 non-null     float64\n",
            " 3   IPM-NM                                              56 non-null     float64\n",
            " 4   Moradia                                             56 non-null     float64\n",
            " 5   Acesso aos serviços de utilidade pública            56 non-null     float64\n",
            " 6   Saúde e alimentação                                 56 non-null     float64\n",
            " 7   Educação                                            56 non-null     float64\n",
            " 8   Acesso a serviços financeiros e padrão de vida      56 non-null     float64\n",
            " 9   Transporte e lazer                                  56 non-null     float64\n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 4.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3IPXVLXL6dHT"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf\n",
        "conf = SparkConf().set(\"spark.executor.memory\", \"2g\")\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RandomFlorestIPMNMSpark\") \\\n",
        "    .config(\"spark.driver.port\", \"7077\") \\\n",
        "    .config(conf=conf) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iPRS4Gj96dHT"
      },
      "outputs": [],
      "source": [
        "pof6_spark = spark.createDataFrame(pof6)\n",
        "\n",
        "selected_columns = ['Moradia', 'Acesso aos serviços de utilidade pública', 'Saúde e alimentação',\n",
        "                    'Educação', 'Acesso a serviços financeiros e padrão de vida', 'Transporte e lazer', 'IPM-NM']\n",
        "df_spark = pof6_spark.select(selected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bQu-JMQz6dHU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "feature_cols = ['Moradia', 'Acesso aos serviços de utilidade pública', 'Saúde e alimentação',\n",
        "                'Educação', 'Acesso a serviços financeiros e padrão de vida', 'Transporte e lazer']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df_spark = vector_assembler.transform(df_spark)\n",
        "\n",
        "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ssLqD4p56dHV"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf_model = RandomForestRegressor(featuresCol=\"features\", labelCol=\"IPM-NM\", numTrees=100, seed=42)\n",
        "\n",
        "model = rf_model.fit(train_data)\n",
        "\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y-l4pLLt6dHV"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"IPM-NM\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "mse_rf = evaluator.evaluate(predictions)\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"IPM-NM\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse_rf = evaluator.evaluate(predictions)\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"IPM-NM\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae_rf = evaluator.evaluate(predictions)\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"IPM-NM\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2_rf = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SY7aK1x6dHW",
        "outputId": "c29e69ae-aaba-471d-92af-90e1416ee56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 3.939212816363966\n",
            "Root Mean Squared Error (RMSE): 1.9847450255294674\n",
            "Mean Absolute Error (MAE): 1.533741693640499\n",
            "R-squared (R²): 0.7684213380529044\n"
          ]
        }
      ],
      "source": [
        "print(f\"Mean Squared Error (MSE): {mse_rf}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_rf}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_rf}\")\n",
        "print(f\"R-squared (R²): {r2_rf}\")\n",
        "\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}